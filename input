BigHAR: Big Data Analytics Framework for Human Activity Recognition
 




1.       Introduction – Lee
An alarming proportion of the US population is overweight. Two out of every three US adults are overweight and one in three obese (Flegal et al. 2010). The trend appears to begin in childhood. One in six children in the US is obese and one in three overweight (Ogden et al. 2010). Obesity increases the risk of a long list of serious illnesses like diabetes, cardiovascular diseases, joint diseases, cancer, sleep disorders and asthma. In fact, the effect of obesity on health has been shown to be worse than that of habitual drinking or smoking (Sturm and Wells 2001). The epidemic of being overweight can be attributed to the deleterious combination of inexpensive high calorie foods and lack of physical activity. One of the root causes is the establishment of sedentary behaviors in childhood that are propagated throughout life (DeMattia et al, 2007). A survey of children between the ages of 8-18 reports that the average recreational time spent on computers rose from 1h 29m in 1999 to 2h 17m (Rideout et al. 2010). 
The growing affordability of smart phones and mobile computing devices has only added to this trend by encouraging prolonged durations of inactivity, often beginning at a young age.
in this paper we present a framework, called “Big Data Analytics Framework for Human Activity Recognition” that aims to turn this challenge into an opportunity for increasing the level of physical activity by creating a framework for active games on mobile computing devices. While there are a few commercial platforms for active gaming such as Nintendo Wii, Microsoft Kinect, these require special dedicated hardware that is not portable. If similar physical action-based games could be played using smart phones and mobile computing devices, this could lead to a higher average rate of metabolism in a wider group of children. If the games are educational in scope, BigHAR has the additional benefit of combining the act of learning with physical exercise. BigHAR can potentially have a tremendous impact on society by encouraging the development of an active lifestyle, beginning at a young age. This can help counter the growing epidemic of obesity. Addressing an important problem of global importance with innovative information technology can enhance the stature of our life.

2.      Related Work – Lee
There has been a lot of work focusing on adaptive software systems that are closely related to our paper. Some work (Wachs, 2011, Lau, 2012) has focused on gesture-based computer applications using artificial intelligence approaches. More recent work (Zhao et al., 2012, Khan et al. 2012) focuses on smartphone specific gestures for mobile applications. A comparative analysis of previously published interactive game research work (Table I) shows that most of these efforts are for use in a non-mobile context, and/or require considerable additional hardware.
Table I. Interactive Education Tools
Tool
Purpose
Features
Alice 2 (Kelleher et al., 2002)
Enable novice programmers to learn basic programming constructs
Creating their own 3D animations and games
Etoys
(Freudenberg et al., 2009)
An interactive multimedia-authoring tool designed for Logo, Smalltalk, Hypercard, and StarLogo.
Based on Squeak programming language; Drag and drop tiles to construct "scripts"; Model the discrete form of differential equations, and support materials and physical phenomenon.
Gamestar Mechanic  (Torres, 2009)
Online game to learn 
the fundamentals of game design by playing roles 
RPG (Role‐Playing Game); Online social networking   
Kodu
(Microsoft Kodu Game)
Interactive programming experiences within a 3D video game
Editing the world and autonomous behaviors of the characters using an iconic programming language; programming with a standard game controller with XBox 360 game console
Scratch
(Maloney et al., 2010)
Creation of their own stories, games, and animations, and share their creations on the web. 
More tinkerable, more meaningful, and more social through programming; Since May 2007,  more than  300,000 projects  (“the YouTube of interactive media”)

There are several prior research efforts in modeling human motion. In contrast to adopting low level processing for motion detection and motion tracking, activity models have been developed for activity awareness and pattern-based discovery of new activities from video sequences (Brezillon, 2003). Another work (Parsia et al., 2004) focuses on the detection of action changes on real-world sequences using human action classification based on a hierarchy of plans that contain a range of precedence relationships, representations of concurrency and other temporal relationships. A hierarchical framework has been reported for classification of human activities for modeling and detecting activities of daily life through data from distributed camera sensors  (Tsarkov et al, 2006). Track-level analysis deals with the gross-level activity patterns of multiple tracks in various wide-area surveillance situations.  In (Haarslev et al, 2003), an ontology was created for a human activity hierarchy with action specification including static pose, dynamic gesture, body-part action, single-person activity, and group interaction.
Table XX. Overview of Gesture based Applications
 
Approaches
Hardware
Applications
Gesture-based Computer
Applications
Gesture recognition, information storage [L12]
Graphic cards, sensors, Bluetooth, infra red
Navigation, Operating, manipulating, Media system


Motion sensing capability, Gesture recognition[L13]
Intend Wii remote, Infra red pen, Camera, Transmitters, Sensors
Photoshop, Video gaming interaction, Education system


Motion sensing, Artificial intelligence, Neural networks  [L15]
Camera, stick figure, transmitters, PC, pressure sensor
Realistic games, web applications, movies
Gesture-based
Mobile Applications
Gesture recognition, Circular gesture [L16]
Portable projection system, Camera, Colored markers, Bluetooth, Sensors
Decision making, Painting, Take and organize pictures, Video annotation, Person blog information


Multi touch gesture interaction with VE, Shake iphone (depending on acceleration), Track accelerometer data  [L21]
iPhone, Accelerometer, ART toolkit, Sensors
Tracking nearby stations, Sallow text messaging


Walk up on surface irrespective of type, Gesture recognition, Pinch and flick [L22]
Rollers in mouse, Camera, Colored markers, Microphone, Sensors
Phone keypad on to the arm, Take a click using gesture, Organize and resize picture, Information retrieval from blogs or website
Touch table & Small computing devices
 
Multi type sensor, pressure sensitive, Data visualization, Graphic gesture recognition [L17]
Heat recognizers, Transmitters, Pressure recognizers, Sensors
Projected drafting table, Lava lamp application, Organize pictures


Sense motion, Gesture recognition, Motion sensitive [L18]
Siftables, Transmitters, Heat recognizers, Infra red ports for communication
Education, Music, Learning process, Live performances, design artifacts, Projectors, Microsoft office application


Logging to violet.net and Store sequences of ztamps [L20]
Mirror, Computer, Nano rabbits, 3Ztamps, LED’s, Sensors
Recognize RFID stamps, Personal and commercial usage
 
In this paper, as part of the BigHAR ontological framework, we propose an initial design of Human Motion Ontology (HMO) based on a RDF/Protégé framework [W5], anatomy concepts from FMA [W6] and WordNet [W7]. The HMO is composed of two major components: component for human body representation, and space for human motion representation. The component concept hierarchy allows us to build a human body with various sizes on each primitive segment and joints to support polymorphism.
The space concept hierarchy supports primitive motions of each moveable body part and a composition of any complex motion from any level of the existing motions. Motions and segment components are connected via joints that control primitive motion constraints. Human movements are almost countless and it is impossible for people to list all moving patterns they can make. The HMO becomes the shared access point for people to update and reuse movement patterns to develop projects on exertional interfaces by detecting significant human actions via sensors. To deal with the issue of additional complexity when adding new activity to model-based techniques, the HMO proposes a complete layer of primitive motions of each human part that allows motion composition at a high level with little effort. Each concept in HMO shall be used to define the temporal and spatial composition of each gesture in HCI-GO.

Table XX. Overview of Gesture/Motion Analysis
Type of Gesture Data
Applications
Gesture Model
Gesture Recognition Approach
Hardware
Accuracy
Ref.
Accelerometer data
 
Commercial applications
Arrow, Check, Circle, Cross, Pigtail, Shake, Square, Star, Throw, Zigzag
HMM and DTW
Wii mote, PC, sensors
Around 97%
[L5]


Photo browsing at home TV
Square, Circle , Roll, Z , Tennis
HMM
Wii mote, PC, sensors
84-94%
 
 [L6]


For developing  common gestures
Cultural dimensions
HMM model
Wii mote
Varies
[L7]
Accelerometer data,  electromyography,
Video data
Google earth, remote interactions for screen display, Research, laboratory
Irrespective of shapes
Eye writer, Sampled gesture, Muscle as sensing, WIMP interaction
Wiimote,
Virtual reality gloves, Web cam, Electric field, Data gloves, Glove pie
90%
[L2]
Video data
 
Temporal and Special segmentation
Digital data
N/A
N/A
79% (K=2) & 94.6%
( K=4,5,6)
[L8]


Daily life
Human gestures, posture gestures and motion recognition
Operation assistance and corner assistance function
Power wheel chair, sensor
Varies
[L9]


Daily life , elder once
Human actions
Indicating and head
Robot
N/A
[L10]


Everyday activities
Kitchen activities
Motion and action
sensors and cameras
92%
 [L11]
Sensor based
Used in concerts, dance applications, dace schools
Hand gestures, arm gesture, leg gesture, body movement
Final orientation
Kinematic al module
Less accuracy of around 8%
[L 4]
Multi modal sensory data
Facebook youtube, video gaming etc
No shapes specific
 
Work stations, main frames, PC,
Low accuracy
[ L3]
Touch input, overlay wrap sequence to gesture, alphabets build in
Control music player, mobile applications, web pages
Alphabets, Pinch and flick
Open source gesture tool kit developed by Google
Touch screen based device, Gesture overlay,
Touch mobile
N/A
[L1]
 

3.       Model
Big Data Analytics Framework for Human Activity Recognition (BigHAR) Framework
The BigHAR Framework is an intelligent framework composed of motion capture, filtering, segmentation, and motion recognition. Based on our preliminary work, we have learned that recognizing gestures is challenging when motions/gestures are involved with the orientation, speed and resulting accuracy significantly dependent on the complexity of gesture models, overlapping gesture models, and variation in gestures between individuals. The BigHAR framework is based on a three-layer architecture: Big data analytics layer, Active Mobile layer, and Game layer. A key component here is the focus on large motions involving the lower body that are likely to increase the expenditure of energy.
3.1. Big Data Analytics layer
The main challenges associated are 1) how to determine the segment of motion data – such as how long, and where the starting and ending points are, 2) how to define the threshold for errors, 3) how to deal with ambiguous movement (intentional vs. accidental), 4) how to deal with continuous data and recognize significant movement in real time, and 5) how to deal with variations of basic movement patterns – differences in movements depending upon individual physical or personal features.

The motion recognition model is designed:
1)  Gesture Training and Recognition: The proposed gesture/motion recognition model is composed of four major steps for training and recognition:
a.  Segmentation

b. Filtering: Each gesture has some essential components that constitute the signature but also include actions at the beginning and end of the gesture that confound its interpretation. We have four types of filters to deal with gestural noise: i) Low pass filter: Removes past and sudden gestures, ii) High pass filter: Detects short and steady gestures, iii) Idle state filter: Detects gestures that are not moved in any way, iv) Motion detect filter: To know whether it is a motion or not.
c. Quantizer using K-means/medoids  to sample data from 3D space. K is determined depending upon a gesture model. Given a K value, each point represents a unique point of X, Y, Z in 3D space. The higher the K value, the more accurate the result, especially in the context of 3D space.
d. Motion Recognition: We use two models: a) Pattern-based Recognition: based on the acceleration data pattern, we will model each movement similar to our approach developed in (Gorla, 2013). This approach is simple and a lightweight-computing approach although it may be domain-specific. b) Hidden Markov Modeling (HMM) with Viterbi training will be used to build gesture models representing a sequence of spatial states in the time domain represented by vectors of X, Y, Z points. The number of states will be initially based on sharp changes in directions and velocity for each gesture. However the model will be revised with the availability of insert and delete states to allow for a refined model that matches each gesture. We balance the computational cost and accuracy in the final selection of HMM versus summarized pattern recognition.
d.  Online Learning for Motion Library: Using the model proposed by Gallaher (1992) gesture will be classified on the basis of number of expressivity parameters such as: i) How fast the gesture is done, ii) The space needed by the gesture, iii) Style relating to personality traits. Gestures can be further categorized by orientation, the individual who performed the gestures, and cultural aspects (T7).


3.2. For the Active Mobile layer, we developed the motion recognition system that was designed for big data using the Hadoop platform. The motion recognition system can recognize the movement of humans based on data from the accelerometer of Wiimote (Nintendo Wii).  The Wiimote has a built in accelerometer that is capable of generating a stream of acceleration values in X, Y and Z directions corresponding to performed motions, with recent models including a gyroscope as well. The skeleton frame is used to represent data of skeletal points of the body.
Space model: The BigHAR framework supports two kinds of board frames using mobile phones: i) virtual floor board and ii) displacement board. The virtual floor board is used to represent the floor board recognized at particular X and Y coordinates within the range of the gaming space, which is the space within the communication rage between mobile phones and Remote Wii (embedded with an accelerometer). Thus, the interpretation of the X and Y coordinates depends on movement force and estimated distance between an original position and a target position. We design virtual floor board to present objects of the proposed gaming application. Each object is associated with its semantics of characters (e.g., mouse, cat) and a scale of points to be awarded when responding to the objects (e.g., catch a mouse) every time it presents itself to gamers. 
Motion model: In the BigHAR platforms, the body position and acceleration of players are expressed in x, y, and z coordinates in motion space (Fig. 3). The x, y, and z axes are the body axes of the accelerometer.
Collaboration Models: We develop models for collaboration in the form of multiple user/device games (to promote engagement) and multiple sensor integration (to increase interactivity). The former will allow multiple users to play a game on the same device or on multiple devices in shared virtual space. In addition to the necessary integration of sensors from multiple users, we also integrate multiple sensors from each user (i.e., one on each foot/ankle) to enhance the level of interaction within the games.
Calorie Model: Player movements will be captured and calorie consumption will be estimated based on heart beat rate and published data about walking, jumping and running. This is an approximation for the accurate but expensive standard of indirect calorimetry.
Gesture Model: Three categories of gestures models are discussed in this paper, (i) shapes : circle, triangle, square, Z, tennis (ii) emotions : kick, stomp, punch, handshake, clap (iii) physical activity : bar lift, bicep curl, chest lat, arm rises, bicycle crunch.
Mobile App model: For this project, we will use mobile phones as a presentation platform (Fig. 4). Player movement will be depicted on the mobile app interface and the status of calorie consumption associated with the movement will be displayed together with points earned from the game.

3.3. Game Layer
Recent studies have confirmed the effectiveness and motivational nature of educational computer games (Yang, 2012; Papastergiou, 2009; Huggard et al., 2006). Students enjoy problem solving and logical thinking through components of gaming regardless of student gender (Taylor, 2006; Denner, 2005). In this project, we develop sample games that involve lower body, especially leg movements.
The Game layer is composed of four models for the BigHAR active interface (Fig. 5).
Player Model: We will design active lifestyle models for the players based on weight control and learning objectives. Each player will be characterized in terms of weight control goal, user profile, and education goal.
Action Model: The movements will be incorporated into the game: walking, jumping, running.
Character Model: The various types of objects (e.g., mouse, cat, trap) will be used to create the movement and education scenarios. The speed of changing board tile can be adjusted depending upon the situations of players or environments.
Scoring Model:  The game will offer the opportunity for individual or group play. Cooperation will be rewarded in the following way:
Individual scores of team members will contribute to a team score.
There will be synergistic bonus points when team members touch the object on the board and destroy it.
A team member can also help in rescue operations when his partner is stuck in a situation like trapped in a blind alley of the maze.
in this paper, we developed two kinds of meta-games that can be used to generate a wide variety of games by combining different models (see above) with content (didactic and/or visual). We develop one example of each.
Destroy objects: The primary objective of this metagame is to link destruction of virtual objects to physical activity. The sample game CatNmouse shall involve a view where the player has to protect mice by using swinging and stomping leg actions to get rid of cats and traps that appear on the screen, with points accruing accordingly. The level of physical activity can be titrated by adjusting the action thresholds needed and the rate of appearance of objects on the screen.
Match objects: The primary objective of this educational metagame is to link didactic concepts to physical activity. In each case, two rapid movements (e.g. stomping feet) will be necessary to correctly spot a pair. Depending on the age of the child, this could be matching alphabets or words to pictures, questions to answers, music note names to position on a staff, synonyms or antonyms. A variety of games could be developed by using different sets of matched images or words. The levels of difficulty can be based on content as well as the rate at which new content appears.

           Architecture - Lee
        Cloudera Platform – Feichen
        Cloudera platform is an open-source Apache Hadoop distribution system to deal with big data processing and analyzing. Most Hadoop ecosystem projects are also supported by Cloudera. To implement our model, we have used HDFS, map-reduce, Hbase, Hue, Pig.

2. HBase Data Model – Feichen, Prakash
                        
2.1. motion data – Accelerometer, gyroscope
                2.2. location data – GPS, proximity
                2.3. time, duration
                2.4. environmental data – temperature, humidity
                2.5. device data – sensor tag, chronos watch, smartphone, etc.
         2.6. activity data – data on gaming
                2.7. user data – gender, age, health condition, etc
    
2.1

In activity recognition using an accelerometer sensor, each gestures is represented  by a patterns of vectors, each gesture representing acceleration in 3 dimensions. We need a system to analyse this vector data to train and well as analyse discrete activities.  Existing systems such as Wiigee[1] and Jahmm[2] library have reported accuracies of 85% - 95% and 97.4% respectively. Different gestures such as running, walking, and cycling can be detected using above systems. 
For detecting the user motions and daily activities, his gestures should be recognised. The easiest way to do this is to use the smart phone in-built acceleration data and gyroscope. But we wanted to record user activity even when the is playing game or performing some other complex activities like Gym, cycling and running. To detect these multitude of activities with the smartphone would be challenging, mainly because the deployment of the smartphone is not flexible enough. 
    2.2 
        Location data is being very critical in tracking the trends of the groups of people in different seasons, disasters and during daily activities. The collection of location data was straight forward using the smart phone device, which has an inbuilt GPS system. Keeping a log of user activity gave us good data to work with. In our project, Android device is built with Google location service API. With such service, we can easily get geolocation data and represent it with latitude and longitude format. 
    2.3    
     Time of any particular activity grouped with location give great insights into user activity on a daily basis. For example, we are able to get time consuming of commuting, running and sleeping of an individual in an convenient way. Interesting recommendations could be generated very easily with time and location data combined.

    2.4 
        Environmental sensor data add an extra bit of features to our data. These include sensor information like ambient humidity, illuminance, ambient pressure, and ambient temperature. From all these information, we are capable to know the accurate temperatures and humidity for a certain user to go out for doing a particular activity, and appropriate suggestion could also be made.

    
    2.5 
        Smart phones provide most of these above data, but the deployment of each devices should be flexible to have a large gesture library in order to detect a wide range of activities. In this case, we considered both Chronos Watch and Sensor Tag from Texas instruments. Since sensor tag was Bluetooth 4.0 , it was the right device to be connected to smart phones and easily deployable to different body parts. Where as the Chronos watch is RF device, which could not save all the sensors data. 

    
    2.6
        We used sensor tag as the primary source of activity recognition for gesture recognition. We were able to recognise different gestures by motion recognition algorithms and built android based games to let users interact using gestures and track these activities as well.

    2.7
        All the above data is specific for each user, hence building a user profile is another consideration. hence we save the user profile details such as Age, height, weight, Gender, Health Condition, BMI for later use.        
    
    
We have used 2 sensing devices for data collection. One being the smart phone and the other Sensor Tag from the Texas Instruments. 

3. HMM – Prakash

3.1. Preprocessing
3.2. Segmentation
3.3. Recognition (k-means/HMM)

3.1
        Accelerometer data collected for activity recognition contains a lot of noise, because of the nature of 3-axis accelerometers built. Hence we had to clean the acceleration data before processing it. We used different filtering techniques to clean the data. 
i.High pass filter to remove acceleration data which is very slow
ii.Low pass filter to remove acceleration data which is very fast(very fast jerk)
iii.Idle detection filter to detect if the device is idle
iv.Gesture detection to detect the gesture activity based on time of device used.
3.2

Having the sensor tag deployed in a particular location on body and trying to detect different activities, we had a major challenge of trying to detect each gesture start and end dynamically. Many system take user inputs to recognise the start of a gesture (example a button press) and similarly for end of the gesture. Since our deployments could be anywhere on the body, this was not feasible. Hence we used the below approach to segment the acceleration data.
where x,y,z is the acceleration data at time t
x1,y1,z1 is the acceleration data at time t1(immediate acceleration data)

d = sqrt( (x1-x)^2 + (y1-y)^2 + (z1-z)^2)

The range of d has to be determined carefully for each application based on the use of application and the activities involved. Generally the values of d to detect a gesture are found be d>0.3 and for the end to be d< 0.1.    
3.3

Recognition of various activities is done by using some of the existing gesture recognition libraries, such as Jahmm and Wiigee. We found the user of Jahmm to be easy for our use. With the right training we have achieved promising accuracy. Results to be posted. Each library used has a set of algorithms to detect the gestures, the basic flow of steps are like below.
1. Cluster the gesture acceleration data into states generated using K-means.
2. Create an initial HMM using these states.
3. Initial HMM models are learnt using Baum-Welch.




Data model design for Hbase:
    
    Architecture : 

(1) Training Architecture

As Figure 1 shows, the basic framework is made up with four models: client accessing model, Hadoop map model, data management model and HMM training with reduce model. All three models work under HDFS. For the first model, we implemented a Restful web service on Cloudera server to upload user data into HDFS. In this URL based service, user needs to provide gesture name, sequence file as parameters, and the output is a key value pair in which key is the gesture name, and value is related sequence data. The second model first reads those key value data as input ( {punch, seq1}, {punch, seq2}, {circle, seq3} and {circle, seq4} in Figure 1), applies Hadoop map function on them and merges sequence data with same key. The output of model 2 is the summary map of each gesture with its sequence data. The third model then reads map result and runs HMM training algorithm with Hadoop reduce function to reduce the gesture with the same and append sequence together for same gesture name. In particular, the key for map could be more specific with device name and activity detection method. For example, key “punch_sensorTag_accelerometer” is different from “punch_chronoWatch_accelerometer”, so we can save training model in a more general manner by adding more features in key. The fourth model is responsible for saving map-reduce results and training models into HBase.

(2) Testing Architecture

As Figure 2 shows, testing framework is divided into two models: data downloading model and HMM testing model. The data downloading model downloads training models from HBase and save it to mobile client. User can choose one or more models to download for testing their data. For example, if user 1 only wants to test circle and punch, he/she just needs to download two models from HBase. The second model then applies HMM testing algorithms on teting data and compares the results with training models. The output with highest probability will be chosen as motion recognition result.

                    Figure 1. Training Framework



                           Figure2. Testing Framework


    
Fig 1.    

Algorithm 1. K-means with Map Reduce
Input: Observation Vector v, Optimal Cluster Number k
Output:: Clusters Group <C>

create KMeansCalculator object KMC with v and k. KMC(k,v)
define map M
define collection list CL
define integer i=1
map
  while (i less than k)
      md = KMC get minimum distance for i
      M insert <md, v>
      i++
  end while
end map

reduce
   for each element in M  
         merge v under same md to get the output M’<md’, v’>
   end for
  
   while (i less than M’ size)         
         CL = KMC get cluster by each <md’, v’> pair
         add CL to <C>
   end while
end reduce
return <C>
        





HMM Map Reduce Training Algorithm

 
Algorithm 1. HMM Map Reduce Training Algorithm
Input: String P which contains <Gesture name G + “token” + sequence S>
Output: Gesture name G and sequence concatenate C Map M2<G, C>
1.   map
2.         define Gesture and sequence Map M1<G,S>
3.                  Iteration<- P StringTokenizer with “token”
4.                  if Iteration hasMoreTokens
5.                      M1 save pair (G, S)
6.          end if
7.   end map
8.                  
9.   reduce
10.      for each e ∈  M1
11.            var a = M1 get value with e
12.               if M2 has key e
13.            var b = M2 get value with e
13.            M2 save pair (e, a+”:”+b)
14.         end if
15.         else
16.             M2 save pair (e, a)
17.        end else
18.   end for
19. HMMTraining (M2)
20.   end reduce
 
 HMM model receives segmented data and then separate data by token value. Then HMM model creates a map and save gesture with sequence data as pair in map function. In reduce function, HMM model combines all sequence data with the same gesture name then runs training function for each gesture to generate different training models.






Filter and Segmentation Map Reduce Algorithm
 
Algorithm 2. Filter and Segmentation Map Reduce Algorithm
Input: Raw Data R with format <x,y,z>
Output: String P which contains <Gesture G’ + “token” + sequence S>
 1.   map
 2.        define Filter F, Segmentator T, Accerometer A, Sequence S,     
 3.       Gesture # G, Gesture and sequence Map M1<G,S>, String P, 
 4.       Decision Tree T contains training model
 5.           for each line in R generated by Device
 6.                 if Device is idle
 7.             F emits null
 8.                end if
 9.                 else
 10.                     F emits List<x,y,z> with G
         end else
 12.          end for
 13. end map
 14.   reduce
 15.      for each G
 for each e ∈  List<x,y,z> 
 17.            var a = List<x,y,z> get value with e
 18.    
 19.                a appends to the end of P
 20.                  P is saved to HDFS
 end for
end for
Extract each G and traverse T to determine gesture name G’
 24.  end reduce



This algorithm happens in gesture recognition part. Sensor device generates raw accelerometer data and sends them to filter. Filter receives these data and read them line by line. By reading data,  filter is able to figure out if device is in idle status or not. If device is in idle, filter emits null data, otherwise filter emits accelerometer data with gesture name. Filter algorithm is running under map function. Map-key is a random number to differentiate gesture and map-value is unidle accelerometer data. Segmentator receives output generated from map function and combines all the unidle accelerometer data together with the same gesture number. Segmentation algorithm is running under reduce function. After segmentation, system extracts each gesture and compares corresponding sequence data by traversing a decision tree to determine the gesture name . 







Scalability of Middleware:

    Proposed approach can be scaled to add new gestures and update existing models. 

An existing model can be updated when new data is collected. But how to check data quality with existing models? For this, the approach we suggest is to send the existing gesture model to user. Training being performed on the user device, will then be compared with the model received, if the new data is close to the existing model then we recognise the training data as good and send this to create an updated model for the intended gesture.
Any new gesture model added by new user to this library can be updated by other users as explained above. The new user should be aware of the gesture he wants to train.


Database table layout :

We found Hbase to best suit our research, the main reason being the sparse nature of the data. User activity is not always daily, hence the data being generated is sparse enough to consider hbase. Each user will have a unique row and each days being a column. Each column will have all the user activity reports generated for that day. These will be relayed back to the user in the Android App.


    
4. Web Services – Feichen
    

4.1. data collection service
The main function of data collection service is to transfer both training and testing text files to sequence file format that HMM algorithm is able to parse. Table 4 shows an URL example about data retrieval..
4.2. activity recognition service
Two steps are included in activity recognition service. The first one is to training different models based on training sequence files generated in data retrieval service. After training process is done, the second step comes to apply testing sequence file on those training models and get prediction results. Table 4 shows an URL example about activity recognition.



Service Name
URL
Data Collection Service
http://134.193.136.127:8080/HMMWS/jaxrs/generic/TestFileOperation/-home-cloudera-punchtest2.txt/-home-cloudera-punchtest.seq 
Activity Recognition Service
http://134.193.136.127:8080/HMMWS/jaxrs/generic/HMMTrainingTest/-home-cloudera-punchtrain.seq:-home-cloudera-rtoltrain.seq:-home-cloudera-ltortrain.seq/punch:ltor:rtol/-home-cloudera-Combination.seq


5. Mobile Application
              5.1. sensors - Prakash
              5.2. data collection app – Prakash
              5.3. motion game app – Prakash
              5.4. data transfer app – Feichen
              5.5. activity report/recommendation app - Prakash

    5.1
A mobile application was developed to collect data from different sensors, which to be used for activity analysis. Smart phone and the Sensor Tag devices were used to generate our data set. We used only the accelerometer sensor available in the sensor tag to collect the required data. 

    5.2
    
    To collect the sensor data from the android device a service is written to log the required data into a file with timestamp. Data include Acceleration, Temperature, and Humidity from the Sensor Tag. GPS, luminous intensity from the Android Smartphone. This  frequency of the data collected varied based on the sensor, the acceleration data from the device has a frequency to 10 Hz, whereas the temperature sensor has a frequency of 1. To overcome this issue, we save the same temperature data for every acceleration recorded. 

    5.3

    Paper suggests a new approach to get users involved in physical activity by playing android games. For this we build an android game, where the user had to burn enough energy to play the game rather than with his fingers. Modifying available open source android game projects and tweaking them to work with the sensor tag was easy. Found the a open source alternative for Flappy Bird *(Flutter Cow) which was modified for our needs. Results to be posted, how much time what is the heart rate.

    5.4
    
Since the collected data cannot be processed on the device for any recommendations and saving the data for future use, we needed to move the sensor data to HDFS*. For transferring the data to HDFS we have built an android app, which would use SSH* to transfer the file to our server and move that to HDFS(run once daily).    

    5.5

On the hdfs data(saved before) we run a recommendation algorithm to pull the required results specific to each user, which would be saved to HBase. Paper found that recommendation system in hadoop is scalable for large sensor data being generated. (will post evaluation on using mysql and Hdfs)    . These recommendations are showed to user with including his activity for each day/month.

Here is how the android screen for activity looks like.


Recommendations are not yet decided, should be based on user activity and location data when compared to other users.

6. Results/evaluation – Feichen/Prakash

Specification: 
Server:

Device: Google Nexus 7
Software: Cloudera, Eclipse, Android ADT
Hadoop 2.0.1
GlassFish
HBase

Evaluation:
    
Accuracy:
    An increase in accuracy of the generated gesture model is observed with increase in training data. The data used for training is of two categories, (i) self trained data and (ii) common train data. Self trained data is generated by a particular user and only compared with specific user’s gestures. Common trained data is the data collected from multiple users performing the same action, the model generated with this data is generic and the accuracy varies from user to user when performing gestures using this model. We tend to increase the accuracy of such a model using large training data sets from multiple users to create the gesture models.

Device Library:
    Proposed approach gives a clear abstraction for different devices. Data generated from different devices is reduced and a gesture model is generated specific for the device. This allows multiple devices to use the existing system. In our paper we have showed two device (i) Sensor Tag (ii) Android device. We can extend this easily to devices like remote wii, chronos watch etc. If data could be generated as expected format, gestures could be generated for each device.
Sample data:

clapping    [ 0.734375 0.671875 -1.03125 ] ;[ 1.140625 0.734375 -0.09375 ] ;[ 1.65625 1.0625 2.0 ] ;[ 0.828125 0.046875 1.171875 ] ;[ 0.921875 0.78125 -0.390625 ] ;[ 1.328125 1.375 -0.59375 ] ;[ 0.296875 -0.328125 -0.171875 ] ;[ 1.328125 -0.078125 -0.03125 ] ;[ 1.328125 -0.078125 0.171875 ] ;[ 1.15625 0.0 0.265625 ] ;[ 0.75 0.015625 0.28125 ] ;[ 0.828125 -0.09375 0.125 ] ;[ 0.9375 0.0 -0.03125 ] ;[ 0.703125 0.21875 -0.0625 ] ;[ 1.234375 0.75 -0.15625 ] ;[ 1.09375 0.5625 -0.046875 ] ;[ 0.9375 0.53125 0.03125 ] ;
punch    [ -0.03125 0.09375 -1.765625 ] ;[ -0.09375 -1.171875 -0.109375 ] ;[ -0.421875 -2.0 -1.625 ] ;[ -0.59375 0.328125 -0.1875 ] ;[ -0.59375 0.328125 -0.734375 ] ;[ -0.203125 0.40625 -0.71875 ] ;[ -0.671875 -0.09375 -1.03125 ] ;[ -0.171875 -0.28125 -1.03125 ] ;[ -0.234375 -0.171875 -1.0625 ] ;[ -0.296875 -0.21875 -0.984375 ] ;[ -0.359375 -0.3125 -0.984375 ] ;[ -0.203125 -0.203125 -0.984375 ] ;

Hadoop vs Non Hadoop:
    We evaluate the time taken for the system to train using model generated from hadoop and using android device to train. We show the accuracy of the two models and time it took to create these models.


This will take some time, since evaluation needs to be done based on the real data generated which is not yet complete.

7. Related work – Lee/Feichen/Prakash
8. Conclusion - Lee


References :

1. Thomas Schl ̈omer, Benjamin Poppinga, Niels Henze and Susanne Boll ,Gesture Recognition with a Wii Controller (2009)
2. 
